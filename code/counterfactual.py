import numpy as np
from util import logsumexp

#%% Pre-calculate feature vectors for suggestions
def get_conditional_logprobs(base_model, context_toks, seq, prefix_logprobs=None):
    import kenlm
    context_toks = ['<s>'] + context_toks
    state, _ = base_model.get_state(context_toks, bos=True)
    possible_word_indices = []
    offset_of_chosen_word = []
    base_logprobs = []

    for i, word in enumerate(seq):
        word_idx = base_model.model.vocab_index(word)
        next_words, logprobs = base_model.next_word_logprobs_raw(state, context_toks[-1] if i == 0 else seq[i-1], prefix_logprobs=prefix_logprobs)

        # At this point we're past the first word, so no more prefix logprobs.
        prefix_logprobs = None

        # Store results.
        #
        # Note that since what we're scoring was always generated by the main
        # model, there will never be an <unk>.
        possible_word_indices.append(np.asanyarray(next_words))
        offset_of_chosen_word.append(next_words.index(word_idx))
        base_logprobs.append(logprobs)

        # Advance the model state.
        new_state = kenlm.State()
        base_model.model.base_score_from_idx(state, word_idx, new_state)
        state = new_state

    return possible_word_indices, offset_of_chosen_word, base_logprobs

def get_features(base_model, context_toks, seq, prefix=None, length_bonus_min_length=6):
    if prefix:
        prefix_logprobs = [(0., prefix)]
    else:
        prefix_logprobs = None
    context_toks = context_toks[-6:]

    try:
        possible_word_indices, offset_of_chosen_word, base_logprobs = get_conditional_logprobs(
            base_model, context_toks, seq, prefix_logprobs=prefix_logprobs)
    except Exception:
        return None

    pos_onehot = np.eye(len(base_model.id2tag))

    features_concat = []
    for logprobs, next_words in zip(base_logprobs, possible_word_indices):
        features_concat.append(np.c_[
            logprobs,
            base_model.word_lengths[next_words] >= length_bonus_min_length,
            pos_onehot[base_model.pos_tags[next_words]],
        ])

    return dict(
        possible_word_indices=possible_word_indices,
        offset_of_chosen_word=np.array(offset_of_chosen_word),
        features_concat=features_concat)


def score_seq_by_word(possible_word_indices, offset_of_chosen_word, features_concat, weights):
    logprobs = []
    for indices, chosen, features in zip(possible_word_indices, offset_of_chosen_word, features_concat):
        logits = np.dot(features, weights)
        chosen_logprob = logits[chosen] - logsumexp(logits)
        logprobs.append(chosen_logprob)
    return logprobs

def get_all_features(contexts, suggestions, base_model):
    return [
        [get_features(base_model, context_toks, sugg_words, prefix) for sugg_words, _ in suggs]
        for (context_toks, prefix), suggs in zip(contexts, suggestions)]

#%% Compute TIP-estimated expected reward given a dataset of suggestion -> acceptances
def compute_expected_reward(weights, features, generation_logprobs, observed_rewards, *, M, num_suggs_offered, max_ir_percentile=95):
    num_samples = len(features)
    res = np.zeros(num_samples)
    importance_ratios = np.zeros(num_samples)
    weights_grad = np.zeros((num_samples, len(weights)))
    for i, (feats, gen_logprob, reward) in enumerate(zip(features, generation_logprobs, observed_rewards)):
        probs_to_use = int(np.ceil(reward+1e-6))
        possible_word_indices = feats['possible_word_indices'][:probs_to_use]
        offset_of_chosen_word = feats['offset_of_chosen_word'][:probs_to_use]
        features_concat = feats['features_concat'][:probs_to_use]
        new_logprob = 0.
        logits_s = []
        for indices, chosen, features in zip(possible_word_indices, offset_of_chosen_word, features_concat):
            logits = np.dot(features, weights)
            lse = logsumexp(logits)
            logits_s.append((logits, lse))
            chosen_logprob = logits[chosen] - lse
            new_logprob += chosen_logprob
        raw_importance_ratio = np.exp(new_logprob - sum(gen_logprob[:probs_to_use]))
        importance_ratio = np.minimum(M, raw_importance_ratio)
        importance_ratios[i] = raw_importance_ratio
        res[i] = reward * importance_ratio

        # Backprop
        if importance_ratio >= M:
            # zero gradient.
            continue
        importance_ratio_adj = reward
        new_logprob_adj = importance_ratio_adj * importance_ratio

        for indices, chosen, features, (logits, lse) in zip(possible_word_indices, offset_of_chosen_word, features_concat, logits_s):
            # Backprop through the softmax
            logits_adj = -np.exp(logits - lse)
            logits_adj[chosen] += 1
            logits_adj *= new_logprob_adj
            weights_grad[i] += np.dot(logits_adj, features)

    if False:
        # Clip 95% bounds.
        max_importance_ratio = np.percentile(importance_ratios, max_ir_percentile)
        valid_samples = importance_ratios < max_importance_ratio
        res = res[valid_samples]
        weights_grad = weights_grad[valid_samples]
    return np.sum(res) / num_suggs_offered, np.sum(weights_grad, axis=0) / num_suggs_offered


class Objective:
    def __init__(self, num_suggs_offered, features, generation_logprobs, observed_rewards, M, regularization):
        self.num_suggs_offered = num_suggs_offered
        self.features = features
        self.generation_logprobs = generation_logprobs
        self.observed_rewards = observed_rewards
        self.M = M
        self.regularization = regularization

    def __call__(self, x):
        reward, reward_grad = compute_expected_reward(x, self.features, self.generation_logprobs, self.observed_rewards, M=self.M, num_suggs_offered=self.num_suggs_offered)
        if False:
            cost = self.regularization * np.dot(x, x) - reward
            grad = 2 * self.regularization * x - reward_grad
        else:
            cost = self.regularization * np.sum(np.abs(x)) - reward
            grad = self.regularization * np.sign(x) - reward_grad
        if x[0] < 0:
            cost += -10. * x[0]
            grad[0] -= 10.
        return cost, grad

    def reward_by_sample(self, x):
        return [compute_expected_reward(x, [feats], [glps], [reward], M=self.M)[0]
            for feats, glps, reward in zip(self.features, self.generation_logprobs, self.observed_rewards)]

#%% Compute TIP by _context_ to estimate true new expected reward.
def contextual_expected_reward_samples(weights, suggestions, features, chosens, rewards):
    res = []
    assert len(suggestions) == len(features) == len(chosens) == len(rewards)
    for suggs, feats, chosen, reward in zip(suggestions, features, chosens, rewards):
        assert reward > 0
        feat = feats[chosen]
        words, gen_probs = suggs[chosen]
        probs_to_use = int(np.ceil(reward+1e-6))
        res.append((sum(gen_probs[:probs_to_use]), sum(score_seq_by_word(weights=weights, **feat)[:probs_to_use]), reward))
    return np.array(res)

def compute_expected_reward_by_context_tip(contextual_expected_reward_samples, M):
    log_importance_ratios = np.array([p - q for q, p, reward in contextual_expected_reward_samples])
    rewards = np.array([reward for q, p, reward in contextual_expected_reward_samples])
    return np.mean(rewards * np.minimum(M, np.exp(log_importance_ratios)))
#%%
def chernoff_bound_on_M(M, delta, n, max_reward):
    return M * max_reward * np.sqrt(np.log(1/delta) / (2*n))

def bounded_estimate_reward(cers, num_suggestions_offered, M, delta):
    qs, ps, rewards = np.array(cers).T
    importance_ratio = np.exp(ps-qs)
    estimated_reward = rewards * np.minimum(importance_ratio, M) * (len(rewards) / num_suggestions_offered)
    mean = np.mean(estimated_reward, axis=0)
    return mean# - chernoff_bound_on_M(M, delta, num_suggestions_offered, np.max(rewards))
